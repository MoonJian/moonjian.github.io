<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>(Paper-Reading) A Comprehensive Analysis of Deep Regression</title>
    <link href="/2022/04/04/Survey-Deep-Regression/"/>
    <url>/2022/04/04/Survey-Deep-Regression/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>Regression could also be formulated as classification, why should we dig into this topic?<blockquote><p>First, there is an inherent trade-off between the complexity of the optimization problem and the accuracy of the method due to the discretization process. Second, in absence of a specific formulation, a confusion between two classes has the same cost independently of the corresponding error in the target space. Last, the discretization method that is employed must be designed specifically for each task, and therefore general-purpose methodologies must be used with care. Thus, we should have a generic vanilla deep regression.</p></blockquote></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>(Paper Reading) Future Research Directions on Domain Generalization</title>
    <link href="/2022/01/05/PR-Domain-Generalization-Future-Directions/"/>
    <url>/2022/01/05/PR-Domain-Generalization-Future-Directions/</url>
    
    <content type="html"><![CDATA[<p>最近在研究领域泛化，想找个方向做点东西，于是整理了最近看的一篇综述<em>Domain Generalization in Vision: A Survey</em><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Domain Generalization in Vision: A Survey">[1]</span></a></sup>，希望能从中有所收获。</p><h1 id="发展方向"><a href="#发展方向" class="headerlink" title="发展方向"></a>发展方向</h1><p>作者从三个方面进行了总结：<strong>模型结构</strong>、<strong>学习方法</strong>和<strong>评价基准（Benchmark）</strong>。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="动态结构（Dynamic-Architectures）"><a href="#动态结构（Dynamic-Architectures）" class="headerlink" title="动态结构（Dynamic Architectures）"></a>动态结构（Dynamic Architectures）</h3><p>在源域训练好后，神经网络的参数通常就固定了下来，充当一个特征提取器，直接应用在目标域中。但是目标域的数据分布可能会和源域差异很大，参数固定的特征提取器可能提取不到有效的特征。针对这个问题，一种可能的解决思路是构建动态神经网络（Dynamical Neural Networks），让网络的结构和权重与输入数据挂钩。</p><blockquote><p>这是我很感兴趣的一个方向，让动态变化的网络结构处理动态变化的数据分布。在领域泛化问题中，就是处理领域的变化， Domain Shift，更进一步，甚至可以用于处理持续领域泛化（Continuous Domain Generalization）问题。但难点在于：1. 选用怎样的动态网络结构？2. 如何处理领域泛化目标域中的无标记数据？3. 如何利用无标记数据动态改变网络结构？4. 是网络结构整体动态变化还是部分动态变化？</p></blockquote><h3 id="自适应归一化层（Adaptive-Normalization-Layers）"><a href="#自适应归一化层（Adaptive-Normalization-Layers）" class="headerlink" title="自适应归一化层（Adaptive Normalization Layers）"></a>自适应归一化层（Adaptive Normalization Layers）</h3><p>自适应归一化层动态改变网络权重（准确地说，是CNN中BN层的权重），不改变网络结构。神经网络BN层可以表示为：</p><script type="math/tex; mode=display">y = \gamma \frac{x-\mu}{\delta} + \beta</script><p>其中，$(\gamma,\beta)$是可学习的参数，$(\mu,\delta)$是训练集上的移动平均和移动标准差，这些参数都和训练数据的分布有关。如何让归一化层的参数自适应目标域中的数据分布，是一个可能的方向。</p><blockquote><p>相比动态结构方法，自适应归一化层方法更加简洁，但是目前已有一篇相关的工作了<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Tent: Fully Test-Time Adaptation by Entropy Minimization">[2]</span></a></sup></p></blockquote><h2 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h2><p>这一部分可以挖的坑是最多的，当然，也是最卷的（笑~</p><h3 id="无领域标签学习（Learning-without-Domain-Labels）"><a href="#无领域标签学习（Learning-without-Domain-Labels）" class="headerlink" title="无领域标签学习（Learning without Domain Labels）"></a>无领域标签学习（Learning without Domain Labels）</h3><p>当前的大多数领域泛化方法在训练时需要知道不同域的领域标签，即域是事先划分好的，域与域之间的界限很明确。但是在现实场景中，域的标签是很难获得和定义的。这个时候，虽然我们有很多标记好的数据，但是却不知道这些数据可以被划分到哪些域中。在这种设定下，当前的SOTA方法很难直接应用。虽然目前已有了一些工作，但是仍不尽如人意。</p><blockquote><p>一开始我并不很清楚这个设定是否有意义，因为这相当于learning in the wild（虽然有标签，但没有域信息），如何定义Target Domain？可能需要再看下综述中提到的几篇论文。</p></blockquote><h3 id="新域数据生成（Learning-to-Synthesize-Novel-Domains）"><a href="#新域数据生成（Learning-to-Synthesize-Novel-Domains）" class="headerlink" title="新域数据生成（Learning to Synthesize Novel Domains）"></a>新域数据生成（Learning to Synthesize Novel Domains）</h3><p>与Out-of-distribution(OOD) Generalization相关的研究<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks">[3]</span></a></sup>表明，增加域的多样性有利于提高OOD检测的泛化性。但是，域是无穷无尽的，不可能收集到所有的域数据。因此，学会如何生成新域数据是一个可能的方向。</p><blockquote><p>这个方向已经有不少工作了，但是问题的实质并没有得到解决，依靠增加训练集的多样性来提高泛化能力，在我看来，成本还是太高，not my taste~</p></blockquote><h3 id="避免学习捷径（Avoiding-Learning-Shortcut）"><a href="#避免学习捷径（Avoiding-Learning-Shortcut）" class="headerlink" title="避免学习捷径（Avoiding Learning Shortcut）"></a>避免学习捷径（Avoiding Learning Shortcut）</h3><p>捷径学习（Shortcut Learning）指的是神经网络偏好学习简单的特征，这些简单特征能在训练集上表现得很好，但很难学到与任务相关的特征<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Shortcut Learning in Deep Neural Networks">[4]</span></a></sup>，从而影响泛化性。例如，区分不同颜色的数字时，神经网络可能更多地学到颜色相关的特征，而不是形状和轮廓相关的特征。这是一种bias，理所当然地会影响泛化性。在领域泛化中，每个域都有自己特定的bias，如果神经网络只学到了每个类的bias，那么泛化性能绝对大打折扣！</p><blockquote><p>初看是一个新颖的角度，但细想，这不就是解决领域泛化中常谈的domain-specific bias问题吗？不过后来再仔细想了下，这里的bias可能比较特殊，是能够引发shortcut learning的bias，是与任务无关但却造成影响的bias，和那些数据不平衡之类的bias有点不同。我对这个方向还是很感兴趣的，得看下文献中是如何定义shortcut的~</p></blockquote><h3 id="因果表征学习（Causal-Representation-Learning）"><a href="#因果表征学习（Causal-Representation-Learning）" class="headerlink" title="因果表征学习（Causal Representation Learning）"></a>因果表征学习（Causal Representation Learning）</h3><blockquote><p>因果推断+表示学习 我赌五毛钱，绝对是未来蓬勃发展的方向，是迈向下一波人工智能浪潮的关键（这里佩服周Sir的眼光！）</p></blockquote><p>当前主流的表示学习方法是为了学到数据$X$到标签$Y$的一个映射$P(Y|X)$，但是这样学得的表示难以处理OOD的数据。一种可能的解决方法是通过生成模型，如autoencoder，直接建模$P(X,Y)$，这可能会提高鲁棒性和泛化性。</p><blockquote><p>表示学习相关的内容是好学的，但因果推断涉及的东西很多，很深，虽然这是一个很好的方向，但本人太菜了，在基础没有打牢之前，我是绝对不会碰的~</p></blockquote><h3 id="利用辅助信息（Exploiting-Side-Information）"><a href="#利用辅助信息（Exploiting-Side-Information）" class="headerlink" title="利用辅助信息（Exploiting Side Information）"></a>利用辅助信息（Exploiting Side Information）</h3><p>辅助信息可以理解为一些有用的场外因素，比如物体检测，大部分人用的是RGB信息，但如果能够得到图像的深度信息RGB-D，那么将提高检测的准确率。在领域泛化中，近期有些工作挖掘了数据的属性信息，比如颜色、形状、条纹等，<strong>这些属性信息是通用的，与domain-specific bias无关</strong>！在zero-shot learning中，挖掘属性信息已经成为了检测新类的一种常用方法。</p><blockquote><p>看完后有一个问题：如何确保利用到的辅助信息与domain-specific bias无关？就算很通用的属性，比如颜色，也要分场景吧？进一步，如果要保证辅助信息的通用性，那么前深度学习时代运用的特征比如HoG是否可以有用武之地？</p></blockquote><h3 id="迁移学习（Transfer-Learning）"><a href="#迁移学习（Transfer-Learning）" class="headerlink" title="迁移学习（Transfer Learning）"></a>迁移学习（Transfer Learning）</h3><p>这里的迁移学习特指Synthetic-to-real场景下的迁移学习，即从真实数据集上训练好的模型，要能够迁移到下游合成的数据上，同时，要能够依然保持真实数据集上学到的知识，说白了，就是持续学习（Continual Learning）。</p><blockquote><p>没搞懂这个场景和持续学习的区别所在，以及它为啥是一个realistic and practical的设定？不过yysy，之前搞过一段时间的持续学习，说不定可以挖坑（笑~</p></blockquote><h3 id="半监督领域泛化（Semi-Supervised-Domain-Generalization）"><a href="#半监督领域泛化（Semi-Supervised-Domain-Generalization）" class="headerlink" title="半监督领域泛化（Semi-Supervised Domain Generalization）"></a>半监督领域泛化（Semi-Supervised Domain Generalization）</h3><p>经典A+B</p><blockquote><p>不说了，看名字都猜得到干啥，很卷的一个方向</p></blockquote><h3 id="开放域泛化（Open-Domain-Generalization）"><a href="#开放域泛化（Open-Domain-Generalization）" class="headerlink" title="开放域泛化（Open Domain Generalization）"></a>开放域泛化（Open Domain Generalization）</h3><p>可以认为是一种新的设定，源域是异质的，即域与域之间不仅数据分布不同，而且含有不同的标记空间，针对目标域，模型要能够识别出见过的类别，并且检测出没有见过的类别。</p><blockquote><p>这是一个和Heterogeneous Domain Generalization和Open-set Recognition都有关的一个问题，难度可想而知。</p></blockquote><h2 id="评价基准"><a href="#评价基准" class="headerlink" title="评价基准"></a>评价基准</h2><h3 id="持续领域泛化学习（Incremental-Learning-Domain-Generalization）"><a href="#持续领域泛化学习（Incremental-Learning-Domain-Generalization）" class="headerlink" title="持续领域泛化学习（Incremental Learning + Domain Generalization）"></a>持续领域泛化学习（Incremental Learning + Domain Generalization）</h3><p>以往领域泛化的工作利用到的训练数据是固定的，不会发生变化，但是在实际场景中，训练数据可能是动态到来的，这个时候，就需要针对源域的持续领域泛化学习。但是，有如下3个问题需要解决：1）在新到来的数据上，怎样fine-tune模型，而不是重新训练一遍？2）怎样避免灾难性遗忘以及过拟合新数据？3）怎样判断新数据对模型泛化性能的影响，有益还是有害？</p><blockquote><p>我认为这是一个很有意义的方向，如果能有一个好的方案，将有很大的现实意义，值得关注！</p></blockquote><h3 id="异质域变化（Heterogeneous-Domain-Shift）"><a href="#异质域变化（Heterogeneous-Domain-Shift）" class="headerlink" title="异质域变化（Heterogeneous Domain Shift）"></a>异质域变化（Heterogeneous Domain Shift）</h3><p>当前领域泛化相关的工作，源域和目标域高度相关，比如只是一些颜色反转、图像旋转之类的，很少考虑到异构变化，比如源域数据是物体的照片、油画和简笔画，但是目标域却是新视角下的图片。</p><blockquote><p>这个设定虽然很难，但却对现实应用很有意义。</p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://ui.adsabs.harvard.edu/abs/2021arXiv210302503Z/abstract#:~:text=%EE%80%80Domain%20Generalization%20in%20Vision%3A%20A%20Survey%EE%80%81.%20%EE%80%80Generalization%EE%80%81%20to,often%20violated%20in%20practice%20due%20to%20%EE%80%80domain%EE%80%81%20shift.">Domain Generalization in Vision: A Survey</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://iclr.cc/virtual/2021/spotlight/3479">Tent: Fully Test-Time Adaptation by Entropy Minimization</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://openreview.net/forum?id=UH-cmocLJC">How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://www.nature.com/articles/s42256-020-00257-z#:~:text=Fig.%201%3A%20Examples%20of%20shortcut%20learning.%20Deep%20neural,pattern%20can%20be%20observed%20in%20many%20real-world%20applications.">Shortcut Learning in Deep Neural Networks</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Tech</category>
      
      <category>DL</category>
      
      <category>DG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(Paper Reading) Adaptive Methods for Real-World Domain Generalization</title>
    <link href="/2021/12/30/PR-Adaptive-Methods-Real-World-DG/"/>
    <url>/2021/12/30/PR-Adaptive-Methods-Real-World-DG/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>建立了一个DG大规模数据集Geo-YFCC，考虑到了长尾分布</li><li>用了最naive的方法ERM，但却取得了非常好的效果</li><li>“In our work, we propose an adaptive classifier that can be adapted to any new domain using very few unlabelled samples without any further training”</li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Core-Idea"><a href="#Core-Idea" class="headerlink" title="Core Idea"></a>Core Idea</h2><p>在测试时，利用少量无标记样本刻画unseen domain信息，然后用其辅助分类。</p><h2 id="Prototypical-Domain-Embeddings"><a href="#Prototypical-Domain-Embeddings" class="headerlink" title="Prototypical Domain Embeddings"></a>Prototypical Domain Embeddings</h2><p>Prototypical Networks真好用！用一个域中图片特征的平均值来刻画域信息，理论基础是Kenerl Mean Embeddings(KME)。训练时的loss是：</p><script type="math/tex; mode=display">p_{\boldsymbol{\theta}}(\mathbf{x} \in D)=\frac{\exp \left(-\left\|\boldsymbol{\mu}(\widehat{D})-\Phi_{\mathcal{D}}(\mathbf{x})\right\|_{2}^{2}\right)}{\sum_{i=1}^{N_{t}} \exp \left(-\left\|\boldsymbol{\mu}\left(\widehat{D}_{i}\right)-\Phi_{\mathcal{D}}(\mathbf{x})\right\|_{2}^{2}\right)}</script><p>其中，$\Phi_{D}(\cdot)$是特征提取网络。</p><h2 id="ERM-on-Augmented-Inputs"><a href="#ERM-on-Augmented-Inputs" class="headerlink" title="ERM on Augmented Inputs"></a>ERM on Augmented Inputs</h2><p>测试时，计算图片特征及特征平均值（域信息），concatenate一下输入分类器。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>构造了两个数据集：Geo-YFCC和LT-ImageNet，数据量都在百万量级，4张V100起步。</p>]]></content>
    
    
    <categories>
      
      <category>Tech</category>
      
      <category>DL</category>
      
      <category>DG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(Paper Reading) 近期关于领域泛化文章的归纳整理</title>
    <link href="/2021/12/22/PR-Domain-Generalization/"/>
    <url>/2021/12/22/PR-Domain-Generalization/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>领域泛化（Domain Generalization, DG），不同于领域自适应（Domain Adaptation, DA），在训练过程中无法接触到目标域的数据，因此比DA更加困难，是一个新坑。</p><h1 id="Test-Time-Classifier-Adjustment-Module-for-Model-Agnostic-Domain-Generalization-T3A"><a href="#Test-Time-Classifier-Adjustment-Module-for-Model-Agnostic-Domain-Generalization-T3A" class="headerlink" title="Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization (T3A)"></a>Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization (T3A)</h1><h2 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights:"></a>Highlights:</h2><ul><li>Source-free, 不存储训练数据</li><li>Back-propagation-free, 在测试阶段不训练</li><li>Can be applied to online inference</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><script type="math/tex; mode=display">\mathbb{S}_{t}^{k}= \begin{cases}\mathbb{S}_{t-1}^{k} \cup\left\{\frac{f_{\boldsymbol{\theta}}(\boldsymbol{x})}{\left\|f_{\boldsymbol{\theta}}(\boldsymbol{x})\right\|}\right\} & \text { if } \hat{y}=y^{k} \\ \mathbb{S}_{t-1}^{k} & \text { else }\end{cases}</script><script type="math/tex; mode=display">\underset{y_{k}}{\arg \max } \gamma_{c}\left(Y=y_{k} \mid f_{\boldsymbol{\theta}}(\boldsymbol{x})\right)=\frac{\exp \left(\boldsymbol{z} \cdot \boldsymbol{c}^{k}\right)}{\sum_{j} \exp \left(\boldsymbol{z} \cdot \boldsymbol{c}^{j}\right)},</script><script type="math/tex; mode=display">\boldsymbol{c}^{k}=\frac{1}{\left|\mathbb{S}^{k}\right|} \sum_{\boldsymbol{z} \in \mathbb{S}_{t}^{k}} \boldsymbol{z}</script><p>简单来说，就是把在不同域上训练好的模型当作一个特征提取器，然后在测试数据上采用一个最近邻分类，需要注意，因为采用了最近邻，所以需要给每个类维持一个类别原型，这个类别原型会随着测试数据的变化而动态变化。</p><h2 id="Learn-from-this-paper"><a href="#Learn-from-this-paper" class="headerlink" title="Learn from this paper"></a>Learn from this paper</h2><ul><li>Try to create a new reasonable setting, and prototype is a breakthrough point.</li></ul><h1 id="Generalization-on-Unseen-Domains-via-Inference-time-Label-Preserving-Target"><a href="#Generalization-on-Unseen-Domains-via-Inference-time-Label-Preserving-Target" class="headerlink" title="Generalization on Unseen Domains via Inference-time Label-Preserving Target"></a>Generalization on Unseen Domains via Inference-time Label-Preserving Target</h1><h2 id="Highlights-1"><a href="#Highlights-1" class="headerlink" title="Highlights"></a>Highlights</h2><ul><li>Use test data to implement complex operations</li></ul><h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBddd2c87b268043cd225ca57a99136692?method=download&amp;shareKey=8fa1a22223aa9b5db2d00ab13a281cfe" alt=""><br>简单来说，在源域上利用对比学习的思路训练一个类别特征耦合的神经网络，关注不同域中相同类别数据的特征一致性，然后在特征空间中训练一个生成模型拟合类别特征的分布。对于测试图片，神经网络提取特征后，利用生成模型找到生成一个和测试图片特征最近的特征，该生成特征类别即为真实类别。</p><h2 id="Learn-from-this-paper-1"><a href="#Learn-from-this-paper-1" class="headerlink" title="Learn from this paper"></a>Learn from this paper</h2><ul><li>对生成模型的这种用法我是第一次见（上图D），再次验证了数据生成真的是刷点利器！但是我对这篇文章是否能完整复现存疑。</li></ul><h1 id="Tent-Fully-Test-time-Adaptation-by-Entropy-Minimization"><a href="#Tent-Fully-Test-time-Adaptation-by-Entropy-Minimization" class="headerlink" title="Tent: Fully Test-time Adaptation by Entropy Minimization"></a>Tent: Fully Test-time Adaptation by Entropy Minimization</h1><h2 id="Highlights-2"><a href="#Highlights-2" class="headerlink" title="Highlights"></a>Highlights</h2><ul><li>“Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classification as the level of corruption increases.”</li><li>Source free, just use test data to optimize the whole model.</li></ul><h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB306c48d84d4a177c3d8747c3252d7671?method=download&amp;shareKey=d1a976e91b02e4869a84496dfcf3f008" alt=""><br>简单来说，在测试阶段，目标函数从CE变为Entropy，并且更新参数时，只更新BN层中的$\gamma$和$\beta$。</p><h2 id="Learn-from-this-paper-2"><a href="#Learn-from-this-paper-2" class="headerlink" title="Learn from this paper"></a>Learn from this paper</h2><ul><li>让我理清楚了DG中各个领域的关系，setting说的好啊！<br><img src="https://note.youdao.com/yws/api/personal/file/WEB1291067c5b889241a9001968705a44e4?method=download&amp;shareKey=71db0eaba91f486d313b808b36a5df71" alt=""></li></ul>]]></content>
    
    
    <categories>
      
      <category>Tech</category>
      
      <category>DL</category>
      
      <category>DG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper</tag>
      
      <tag>summary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(Paper Reading) Efficient Deep Learning Survey A Survey on Making Deep Learning Models Smaller, Faster, and Better</title>
    <link href="/2021/12/04/PR-Efficient-Deep-Learning-Survey/"/>
    <url>/2021/12/04/PR-Efficient-Deep-Learning-Survey/</url>
    
    <content type="html"><![CDATA[<p>Published by Google Research.</p><h2 id="1-Main-challenges-in-deep-learning-areas"><a href="#1-Main-challenges-in-deep-learning-areas" class="headerlink" title="1. Main challenges in deep learning areas"></a>1. Main challenges in deep learning areas</h2><ul><li><strong>Sustainable Server-Side Scaling</strong>. deploying and letting inference run for over a long period of time could still turn out to be expensive in terms of consumption of server-side RAM, CPU, etc.</li><li><strong>Enabling On-Device Deployment</strong>. Certain deep learning applications need to run real time on IoT and smart devices (where the model inference happens directly on the device), for a multitude of reasons (privacy, connectivity, responsiveness).</li><li><strong>Privacy &amp; Data Sensitivity</strong>. Being able to use as little data as possible for training is critical when the user-data might be sensitive.</li><li><strong>New Applications</strong>. Certain new applications offer new constraints (around model quality<br>or footprint) that existing off-the-shelf models might not be able to address.</li><li><strong>Explosion of Models</strong>. While a singular model might work well, training and/or deploying multiple models on the same infrastructure (colocation) for different applications might end up exhausting the available resources. (Multi-task learning?)</li></ul><p>Specifically, the common theme around the above challenges is <em>efficiency</em>, which can be categorized into two aspects:</p><ul><li>Inference Efficiency</li><li>Training Efficiency</li></ul><p>Our goal is to train and deploy pareto-optimal models with respect to model quality and its footprint.</p><h2 id="2-A-mental-model"><a href="#2-A-mental-model" class="headerlink" title="2. A mental model"></a>2. A mental model</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcfe8440e5764dc0248ff1c33066dce7d?method=download&amp;shareKey=4407939a456a1e27e2b393d80cca4a52" alt="A mental model for thinking about algorithms, techniques, and tools related to efficiency in Deep Learning"></p><ul><li><strong>Compression Techniques</strong>. Pruning, quantization etc.</li><li><strong>Learning Techniques</strong>. Distillation etc.</li><li><strong>Automation</strong>. HPO, NAS etc.</li><li><strong>Efficient Architectures</strong>. Some fundamental blocks, such as conv, attention.</li><li><strong>Infrastructure</strong>. TF, Pytorch etc.</li></ul><h2 id="3-Landscape-of-Efficient-Deep-Learning"><a href="#3-Landscape-of-Efficient-Deep-Learning" class="headerlink" title="3. Landscape of Efficient Deep Learning"></a>3. Landscape of Efficient Deep Learning</h2><h3 id="3-1-Compression-Techniques"><a href="#3-1-Compression-Techniques" class="headerlink" title="3.1 Compression Techniques"></a>3.1 Compression Techniques</h3><p>In some cases if the model is over-parameterized, these techniques can improve model generalization.</p><ul><li><strong>Pruning</strong>, prefer structured pruning</li><li><strong>Quantization</strong>,  <ul><li><strong>Weight quantization</strong>, we can map the minimum weight value $(𝑥_{𝑚𝑖𝑛})$ in that matrix to 0, and the maximum value (𝑥𝑚𝑎𝑥 ) to $2^{b} − 1$ (where 𝑏 is the number of bits of precision, and 𝑏 &lt; 32). XNOR-Net, Binarized Neural Networks and others use 𝑏 = 1, and thus have weight matrices which just have two possible values 0 or 1, and the quantization function there is simply the $\sign(𝑥)$ function (assuming the weights are symmetrically distributed around 0). Binary quantization <strong>still need support from the underlying hardware</strong>. </li><li><strong>Activation quantization</strong>, This means all intermediate layer inputs and outputs are also in fixed-point, and there is no need to dequantize the weight matrices since they can be used directly along with the inputs. </li><li><font color=red><strong>Quantization-aware training (QAT)</strong></font>, post-training quantization leads to quality loss inference. <strong>Fake quantization</strong>, the training happens in floating-point but the forward-pass simulates the quantization behavior during inference.<br><img src="https://note.youdao.com/yws/api/personal/file/WEB37a680e93db48a8953105f44a62c1327?method=download&amp;shareKey=659a4e53a58a749caf8e6403db38bf16" alt=""><br>QAT is good, but tools like TF Lite have made it easy to rely on post-training quantization. For performance reasons, it is best to consider the common operations that follow a typical layer such as Batch-Norm, Activation, etc. and ‘fold’ them in the quantization operations.</li></ul></li><li>Other Compression Techniques. There are other compression techniques like Low-Rank<br>Matrix Factorization, K-Means Clustering, Weight-Sharing etc. which are also actively being used for model compression and might be suitable for further compressing hotspots in a model.</li></ul><h3 id="3-2-Learning-Techniques"><a href="#3-2-Learning-Techniques" class="headerlink" title="3.2 Learning Techniques"></a>3.2 Learning Techniques</h3><ul><li><strong>Distillation</strong>, transfer ensembled models in weakly supervised learning to a smaller model (2006). Knowledge distillation, in my opinion, the large model provides informative relations among classes. Strategies for intermediate-layer distillation have also shown to be effective in the case of complex networks.</li><li><p><strong>Data augmentation</strong> (training efficiency), various transformations</p><ul><li><strong>label-invariant transformations</strong>, <em>e.g.</em>, flipping, cropping, rotations.</li><li><strong>Label-Mixing transformations</strong>, Mixup, The intuition is that the model should be<br>able to extract out features that are relevant for both the classes.</li><li><strong>Data-Dependent transformations</strong>, In this case, transformations are chosen such that they maximize the loss for that example [56], or are adversarially chosen so as to fool the classifier.</li><li><strong>Synthetic sampling</strong>, SMOTE, GAN</li><li><p><strong>Composition of transformations</strong>, combing above methods</p><p>Auto-Augment sounds practical…</p></li></ul></li><li><strong>Self-Supervised Learning</strong>, fine-tuning models pre-trained with Self-Supervised learning<br>are data-efficient (they converge faster, attain better quality for the same amount of labeled data when compared to training from scratch, etc.) Contrastive learning is effective. <strong>SSL provides a good pre-trained model for data-limited scenarios</strong>.</li></ul><h3 id="3-3-Automation"><a href="#3-3-Automation" class="headerlink" title="3.3 Automation"></a>3.3 Automation</h3><p>The trade-off is that these methods might require large computational resources, and hence have to be carefully applied.</p><ul><li><strong>Hyper-Parameter Optimization</strong>, Grid Search, Random Search (May not effective for high-dimensional space), Bayesian Optimization (it also makes the search sequential, though it is possible to run multiple trials in parallel, overall it will lead to some wasted trials.), Population based training (similar to evolutionary approaches), <strong>MAB algorithms</strong></li><li><strong>NAS</strong>, search space, search algorithm &amp; state, evaluation strategy. From single target accuracy to multi-goals, such as latency.</li></ul><h3 id="3-4-Efficient-Architectures"><a href="#3-4-Efficient-Architectures" class="headerlink" title="3.4 Efficient Architectures"></a>3.4 Efficient Architectures</h3><ul><li><strong>Depth-Separable Convolution</strong>, classical<br><img src=https://note.youdao.com/yws/api/personal/file/WEB94edd371460c9ac3912f34b0f9f85db3?method=download&shareKey=7715f1dfe158aeda2bcfd32dd843a0a3 style="zoom: 50%;" /></li><li><strong>Attention Mechanism &amp; Transformer Family</strong>, very hot!</li><li><strong>Random Projection Layers &amp; Models</strong>, The core-benefit of the projection operation when compared to embedding tables is 𝑂(𝑇 ) space required instead of 𝑂(𝑉 .𝑑) (𝑇 seeds required for 𝑇 hash functions). On the other hand, random-projection computation is 𝑂(𝑇 ) too v/s 𝑂(1) for embedding table lookup. Hence, the projection layer is clearly useful when model size is the primary focus of optimization. Reduce the memory, while increasing latency.</li></ul><h3 id="3-5-Infrastructure"><a href="#3-5-Infrastructure" class="headerlink" title="3.5 Infrastructure"></a>3.5 Infrastructure</h3><ul><li>TF Lite</li><li>PyTorch Mobile</li></ul>]]></content>
    
    
    <categories>
      
      <category>Tech</category>
      
      <category>DL</category>
      
      <category>Compression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper</tag>
      
      <tag>google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github + Hexo 搭建博客踩坑指南</title>
    <link href="/2021/11/03/github-hexo-write-blog/"/>
    <url>/2021/11/03/github-hexo-write-blog/</url>
    
    <content type="html"><![CDATA[<h2 id="开博语"><a href="#开博语" class="headerlink" title="开博语"></a>开博语</h2><p>之前是用阿里云的服务器搭建博客，但学生优惠用完了，年费太贵，遂白嫖GitHub重新搭建博客。回顾之前的写作内容，深感个人输出能力极其欠缺，故借再开博客之机，重整旗鼓，用心写作，以练世事之洞察、情理之分析、表达之逻辑！</p><h2 id="搭建博客中的坑"><a href="#搭建博客中的坑" class="headerlink" title="搭建博客中的坑"></a>搭建博客中的坑</h2><p>Github+Hexo的组合网上已有许多资料，我参考的是这份知乎高赞教程<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Github+Hexo搭建个人网站详细教程-知乎高赞-吴润">[1]</span></a></sup>，但在具体搭建过程中依然踩了不少坑，我将填这些坑的方案整理了一下，你如若也踩到了相似的坑，那也算是有缘~</p><ul><li>你需要再github上创建一个新repo，这个repo的名字是：<strong>用户名</strong>.github.io，<strong>一定得是你github的用户名</strong></li><li>配置git和hexo，见<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Github+Hexo搭建个人网站详细教程-知乎高赞-吴润">[1]</span></a></sup></li><li><code>hexo d</code>推送网站，需要输入你的用户名和密码，然后可能会出现如下问题：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Support <span class="hljs-keyword">for</span> password authentication was removed on August 13, 2021. Please use a personal access token instead.<br></code></pre></td></tr></table></figure>这是由于github现在已经不支持username-passwd这套认证了，取而代之的是个人token认证<br>解决方案是增添Windows普通凭据，参考<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="github_token">[2]</span></a></sup></li><li>即使token配置成功了，<code>hexo d</code>仍会出现问题：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">unable to access <span class="hljs-string">&#x27;https://github.com/XXX/XXX.github.io.git/&#x27;</span>: Failed to connect to github.com port 443: Timed out<br></code></pre></td></tr></table></figure>这是墙的问题，解决方案就是全局proxy，参考<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="当Hexo部署遇到墙">[3]</span></a></sup></li><li>你推送网站成功了，但在浏览器上发现没啥变化，一点动静都没有。这很可能是github仓库的问题，推送的分支是master，但是github page上的是main分支。解决办法分两步：1. 进入repo-&gt;settings-&gt;pages，将source branch修改为master 2. 进入repo-&gt;settings-&gt;branches，修改默认分支为master（其实第2步可以不要）</li><li>如果你有自己的域名想绑定，参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Github+Hexo搭建个人网站详细教程-知乎高赞-吴润">[1]</span></a></sup>，记住一定有CNAME文件，如果只在repo-&gt;settings-&gt;pages中设置custom domain，<code>hexo d</code>会刷新让其无效化</li><li>嫌默认主题太丑，我更换了fluid主题<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="fluid主题">[4]</span></a></sup>，想添加评论功能的话，参考<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="utterance评论管理">[5]</span></a></sup>，想<code>hexo new page XXX</code>新建页，并在主页上显示的话，要在theme _config的menu下添加XXX</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/26625249">Github+Hexo搭建个人网站详细教程-知乎高赞-吴润</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://stackoverflow.com/questions/68775869/support-for-password-authentication-was-removed-please-use-a-personal-access-to">github_token</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://z.arlmy.me/posts/hexo/Hexo_DeployMeetsGFW/">当Hexo部署遇到墙</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://github.com/fluid-dev/hexo-theme-fluid">fluid主题</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://liaoyq.club/2020/04/03/hexo-fluid%E6%B7%BB%E5%8A%A0utterances%E8%AF%84%E8%AE%BA%E5%8A%9F%E8%83%BD/">utterance评论管理</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://segmentfault.com/q/1010000006864722">hexo_ssl_error</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
      <category>blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
      <tag>pit</tag>
      
      <tag>github</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
